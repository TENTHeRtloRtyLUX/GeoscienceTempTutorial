{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a881fd",
   "metadata": {},
   "source": [
    "## Simple example - Temperature Only Analysis\n",
    "This notebook shows how to train model for temperature data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e54843ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy\n",
    "import torch\n",
    "\n",
    "sys.path.append('../src/')\n",
    "from DatasetUS import *\n",
    "from TrainDiffusion import *  # UNCOMMENTED - Now using Diffusion training\n",
    "import Network  # Import Network module for EDMPrecond\n",
    "# from TrainUnet import *  # COMMENTED OUT - Switching to Diffusion training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ea9cf8",
   "metadata": {},
   "source": [
    "This example can be run on a laptop but won't train the network very well. We will train with just a small subset of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84259f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select years to train and validate\n",
    "train_year_start = 1953\n",
    "train_year_end = 1955\n",
    "\n",
    "valid_year_start = 1956\n",
    "valid_year_end = 1957"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046b82d1",
   "metadata": {},
   "source": [
    "Set up training hyperparameters. We will only run for 10 epochs and we will use the cpu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f67c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening files\n",
      "All files accessed. Creating tensors\n",
      "Original data shape: torch.Size([720, 128, 256])\n",
      "Weekly data shape: torch.Size([103, 128, 256]) (86% reduction)\n",
      "Original time steps: 720\n",
      "Weekly time steps: 103 (86% reduction)\n",
      "torch.Size([1])\n",
      "tensor([259.5524]) tensor([310.5276])\n",
      "Opening constant variables file (e.g. land-sea mask, topography)\n",
      "Normalize z\n",
      "Mean:4599.646526826994, Std6220.799692544967\n",
      "Dataset initialized.\n",
      "Opening files\n",
      "All files accessed. Creating tensors\n",
      "Original data shape: torch.Size([360, 128, 256])\n",
      "Weekly data shape: torch.Size([52, 128, 256]) (86% reduction)\n",
      "Original time steps: 360\n",
      "Weekly time steps: 52 (86% reduction)\n",
      "torch.Size([1])\n",
      "tensor([259.5524]) tensor([310.5276])\n",
      "Opening constant variables file (e.g. land-sea mask, topography)\n",
      "Normalize z\n",
      "Mean:4599.646526826994, Std6220.799692544967\n",
      "Dataset initialized.\n"
     ]
    }
   ],
   "source": [
    "## Select hyperparameters of training\n",
    "batch_size = 4  # Reduced from 16 to fit RTX 4050 (6GB VRAM)\n",
    "learning_rate = 1e-4\n",
    "accum = 8\n",
    "\n",
    "# Run training for small number of epochs \n",
    "num_epochs = 2        \n",
    "\n",
    "# Define device\n",
    "torch.cuda.set_device(0)  # Use GPU 1 as default\n",
    "device = 'cuda'  # Will use GPU 1\n",
    "# if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# define the ml model - NOW USING DIFFUSION MODEL\n",
    "# Note: EDMPrecond concatenates noisy image (1 channel) + conditional image (3 channels) = 4 channels total\n",
    "diffusion_model = Network.EDMPrecond((256, 128), 4, 1, label_dim=2)  # UNCOMMENTED - Diffusion model (4 input channels)\n",
    "diffusion_model.to(device)\n",
    "# unet_model = UNet((256, 128), 3, 1, label_dim=2, use_diffuse=False)  # COMMENTED OUT - Switching to Diffusion\n",
    "\n",
    "# define the datasets\n",
    "datadir = \"../data/\"\n",
    "dataset_train = UpscaleDataset(datadir, year_start=train_year_start, year_end=train_year_end,\n",
    "                               constant_variables=[\"lsm\", \"z\"])\n",
    "\n",
    "dataset_test = UpscaleDataset(datadir, year_start=valid_year_start, year_end=valid_year_end,\n",
    "                              constant_variables=[\"lsm\", \"z\"])\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dataloader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a0255e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 13\n",
      "Input shape: torch.Size([4, 3, 128, 256])\n",
      "Target shape: torch.Size([4, 1, 128, 256])\n",
      "Expected input channels: 3 (1 temp + 2 constants)\n",
      "Actual input channels: 3\n",
      "Model expects: 3 channels\n",
      "\\nDataset configuration:\n",
      "- Temperature variables: ['temp']\n",
      "- Number of temperature variables: 1\n",
      "- Constant variables: ['lsm', 'z']\n",
      "- Expected total channels: 3 (temp + constants)\n",
      "\\nWeekly Data Subsampling:\n",
      "- Original data: Daily (365 days/year)\n",
      "- Weekly data: Every 7th day (52 days/year)\n",
      "- Data reduction: 86% (365 → 52 days)\n",
      "- Expected training speed: 7x faster\n",
      "- Training samples (weekly): 156\n",
      "- Validation samples (weekly): 104\n"
     ]
    }
   ],
   "source": [
    "print(len(dataloader_train), len(dataloader_test))\n",
    "\n",
    "# Debug: Check the actual data shapes\n",
    "batch = next(iter(dataloader_train))\n",
    "print(\"Input shape:\", batch[\"inputs\"].shape)\n",
    "print(\"Target shape:\", batch[\"targets\"].shape)\n",
    "print(\"Expected input channels: 3 (1 temp + 2 constants)\")\n",
    "print(\"Actual input channels:\", batch[\"inputs\"].shape[1])\n",
    "print(\"Model expects:\", 3, \"channels\")\n",
    "\n",
    "# Check what channels we actually have\n",
    "print(\"\\\\nDataset configuration:\")\n",
    "print(\"- Temperature variables:\", dataset_train.varnames)\n",
    "print(\"- Number of temperature variables:\", dataset_train.n_var)\n",
    "print(\"- Constant variables:\", [\"lsm\", \"z\"])\n",
    "print(\"- Expected total channels:\", dataset_train.n_var + 2, \"(temp + constants)\")\n",
    "\n",
    "# WEEKLY DATA SUBSAMPLING INFO\n",
    "print(\"\\\\nWeekly Data Subsampling:\")\n",
    "print(\"- Original data: Daily (365 days/year)\")\n",
    "print(\"- Weekly data: Every 7th day (52 days/year)\")\n",
    "print(\"- Data reduction: 86% (365 → 52 days)\")\n",
    "print(\"- Expected training speed: 7x faster\")\n",
    "print(\"- Training samples (weekly):\", (train_year_end - train_year_start + 1) * 52)\n",
    "print(\"- Validation samples (weekly):\", (valid_year_end - valid_year_start + 1) * 52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23a98be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory: 6.4 GB\n",
      "GPU Memory Allocated: 0.39 GB\n",
      "GPU Memory Cached: 0.40 GB\n"
     ]
    }
   ],
   "source": [
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check GPU memory\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "print(f\"GPU Memory Cached: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "\n",
    "scaler = torch.amp.GradScaler('cuda')  # Updated for newer PyTorch\n",
    "\n",
    "# define the optimiser - NOW USING DIFFUSION MODEL\n",
    "optimiser = torch.optim.AdamW(diffusion_model.parameters(), lr=learning_rate)  # UNCOMMENTED - Using Diffusion\n",
    "# optimiser = torch.optim.AdamW(unet_model.parameters(), lr=learning_rate)  # COMMENTED OUT - Using Diffusion\n",
    "\n",
    "# Define the tensorboard writer\n",
    "writer = SummaryWriter(\"./runs_diffusion\")  # UNCOMMENTED - Using Diffusion\n",
    "# writer = SummaryWriter(\"./runs_unet\")  # COMMENTED OUT - Using Diffusion\n",
    "\n",
    "# define loss function - EDM Loss for Diffusion\n",
    "loss_fn = EDMLoss()  # UNCOMMENTED - Using Diffusion\n",
    "# loss_fn = torch.nn.MSELoss()  # COMMENTED OUT - Using Diffusion\n",
    "\n",
    "# train the model\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2c8cf9",
   "metadata": {},
   "source": [
    "Start the training loop using Diffusion framework with weekly data. The plots generated will show the coarse res, the predicted, and the truth for a few samples and for different variables. At the start of training the first two columns (coarse res and predicted) look similar. Towards the end of the training, the last two columns (predicted and truth) should look similar.\n",
    "\n",
    "**Note**: Diffusion training uses iterative denoising (40 steps) for sampling, which is slower but produces higher quality results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0abfebf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train :: Epoch: 0:   4%|▍         | 1/26 [00:05<02:08,  5.12s/it]c:\\Users\\khosl\\OneDrive\\Documents\\GitHub\\ClimateDiffuse-main\\examples\\../src\\TrainDiffusion.py:62: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Train :: Epoch: 0:   4%|▍         | 1/26 [00:06<02:37,  6.30s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [128, 3, 3, 3], expected input[4, 4, 128, 256] to have 3 channels, but got 4 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m torch.cuda.empty_cache()\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Use Diffusion training step\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m epoch_loss = \u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdiffusion_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m losses.append(epoch_loss)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Use Diffusion sampling (iterative denoising)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\khosl\\OneDrive\\Documents\\GitHub\\ClimateDiffuse-main\\examples\\../src\\TrainDiffusion.py:63\u001b[39m, in \u001b[36mtraining_step\u001b[39m\u001b[34m(model, loss_fn, optimiser, data_loader, scaler, step, accum, writer, device)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# forward unet\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.cuda.amp.autocast():\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     loss = \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mconditional_img\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcondition_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m     loss = torch.mean(loss)\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# backpropagation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\khosl\\OneDrive\\Documents\\GitHub\\ClimateDiffuse-main\\examples\\../src\\TrainDiffusion.py:24\u001b[39m, in \u001b[36mEDMLoss.__call__\u001b[39m\u001b[34m(self, net, images, conditional_img, labels, augment_pipe)\u001b[39m\n\u001b[32m     22\u001b[39m y, augment_labels = augment_pipe(images) \u001b[38;5;28;01mif\u001b[39;00m augment_pipe \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (images, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     23\u001b[39m n = torch.randn_like(y) * sigma\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m D_yn = \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditional_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m           \u001b[49m\u001b[43maugment_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43maugment_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m loss = weight * ((D_yn - y) ** \u001b[32m2\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\khosl\\OneDrive\\Documents\\GitHub\\ClimateDiffuse-main\\examples\\../src\\Network.py:381\u001b[39m, in \u001b[36mEDMPrecond.forward\u001b[39m\u001b[34m(self, x, sigma, condition_img, class_labels, force_fp32, **model_kwargs)\u001b[39m\n\u001b[32m    378\u001b[39m c_in = \u001b[32m1\u001b[39m / (\u001b[38;5;28mself\u001b[39m.sigma_data ** \u001b[32m2\u001b[39m + sigma ** \u001b[32m2\u001b[39m).sqrt()\n\u001b[32m    379\u001b[39m c_noise = sigma.log() / \u001b[32m4\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m F_x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_in\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43min_img\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mnoise_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mc_noise\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m.to(dtype)\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m F_x.dtype == dtype\n\u001b[32m    385\u001b[39m D_x = c_skip * x + c_out * F_x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\khosl\\OneDrive\\Documents\\GitHub\\ClimateDiffuse-main\\examples\\../src\\Network.py:324\u001b[39m, in \u001b[36mUNet.forward\u001b[39m\u001b[34m(self, x, noise_labels, class_labels, augment_labels)\u001b[39m\n\u001b[32m    322\u001b[39m skips = []\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.enc.values():\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m     x = block(x, emb) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(block, UNetBlock) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    325\u001b[39m     skips.append(x)\n\u001b[32m    327\u001b[39m \u001b[38;5;66;03m# Decoder.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\khosl\\OneDrive\\Documents\\GitHub\\ClimateDiffuse-main\\examples\\../src\\Network.py:87\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     85\u001b[39m         x = torch.nn.functional.conv2d(x, f.tile([\u001b[38;5;28mself\u001b[39m.in_channels, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m]), groups=\u001b[38;5;28mself\u001b[39m.in_channels, stride=\u001b[32m2\u001b[39m, padding=f_pad)\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m         x = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mw_pad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m b \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     89\u001b[39m     x = x.add_(b.reshape(\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m))\n",
      "\u001b[31mRuntimeError\u001b[39m: Given groups=1, weight of size [128, 3, 3, 3], expected input[4, 4, 128, 256] to have 3 channels, but got 4 channels instead"
     ]
    }
   ],
   "source": [
    "for step in range(num_epochs):\n",
    "    # Clear GPU memory before each epoch\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Use Diffusion training step\n",
    "    epoch_loss = training_step(\n",
    "        diffusion_model, loss_fn, optimiser, dataloader_train, scaler, step,\n",
    "        accum, writer, device=device)\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "    # Use Diffusion sampling (iterative denoising)\n",
    "    (fig, ax), (base_error, pred_error) = sample_model(\n",
    "        diffusion_model, dataloader_test, device=device)\n",
    "    plt.show()\n",
    "\n",
    "    writer.add_scalar(\"Error/base\", base_error, step)\n",
    "    writer.add_scalar(\"Error/pred\", pred_error, step)\n",
    "    \n",
    "    # Clear memory after each epoch\n",
    "    torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
